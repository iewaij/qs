{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access to different databases\n",
    "import pandas_datareader as web\n",
    "import quandl as quandl\n",
    "import wrds as wrds\n",
    "\n",
    "# storage and operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# statistics and regression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from scipy.stats.mstats import winsorize\n",
    "import quantstats as qs\n",
    "from scipy.stats import norm\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.axis import Axis\n",
    "import matplotlib.dates as mdates \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from ta.momentum import KAMAIndicator\n",
    "from ta.momentum import PercentagePriceOscillator\n",
    "from ta.momentum import ROCIndicator\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import AroonIndicator\n",
    "from ta.trend import EMAIndicator\n",
    "from ta.trend import MACD\n",
    "from pandas.tseries.offsets import BDay\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# unzipping zip-files\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeaa870",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_stocks = Path(r'C:\\Users\\Strahinja\\Desktop\\books_Germany\\Quant\\Group_Project\\qs\\data\\permno_selection.csv')\n",
    "df_input_stocks = pd.read_csv(Path_stocks)\n",
    "Path_linker = Path(r'C:\\Users\\Strahinja\\Desktop\\books_Germany\\Quant\\Group_Project\\qs\\data\\daily_permno_secid_cusip_link.csv.zip')\n",
    "zip_file = ZipFile(Path_linker)\n",
    "df_link_permno_secid = pd.read_csv(zip_file.open('daily_permno_secid_cusip_link.csv'))\n",
    "Path1 = Path(r'C:\\Users\\Strahinja\\Desktop\\books_Germany\\Quant\\Homework_1\\data_snp500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_stock_data(df_input, date_start, date_end, wrds_username):\n",
    "    # Download stock data\n",
    "    ## establish WRDS connection\n",
    "    db = wrds.Connection(wrds_username=wrds_username)\n",
    "    \n",
    "    # create query to load the returns, prices and shares outstanding for S&P 500 companies from 1999/01\n",
    "    sql_wrds = \"\"\"\n",
    "            select distinct date, \n",
    "                            permno,\n",
    "                            cusip, \n",
    "                            ret, \n",
    "                            abs(prc) as prc, \n",
    "                            shrout,\n",
    "                            abs(prc)*shrout/1000 as mktval\n",
    "            from crsp.dsf \n",
    "            where permno in %(permno)s and date>=%(start)s and date<=%(end)s\n",
    "            \"\"\"\n",
    "\n",
    "    # define the parameters, i.e. only those companies (permno), which were part of the S&P 500\n",
    "    # in the timeframe 2000/01 until 2020/12\n",
    "    params = {}\n",
    "    params['start'] = date_start\n",
    "    params['end'] = date_end\n",
    "    params['permno'] = tuple(df_input.permno.unique().astype(str))\n",
    "\n",
    "    # retrieve the data from wrds\n",
    "    df_stock_data = db.raw_sql(sql_wrds, params = params)\n",
    "\n",
    "    # change type of entries in the columns start and ending\n",
    "    df_stock_data['date']  = pd.to_datetime(df_stock_data['date'])\n",
    "    \n",
    "    # shift market_val by 1 day\n",
    "    df_stock_data.loc[:,'mktval_shifted'] = df_stock_data.groupby(['permno'])['mktval'].shift(1)\n",
    "    \n",
    "    # dropna\n",
    "    df_stock_data.dropna(inplace=True)\n",
    "    \n",
    "    # calculate weight of each stock on each day\n",
    "    ## dataframe with total market value on each day\n",
    "    df_ttl_mkcap = df_stock_data[['date', 'mktval_shifted']].groupby('date').sum().rename(columns={\"mktval_shifted\": \"ttl_mktval\"})\n",
    "    ## merge with df_stock_data\n",
    "    df_stock_data = pd.merge(df_stock_data, df_ttl_mkcap, how='left', on=['date'])\n",
    "    ## calculate weightage\n",
    "    df_stock_data['weightage_pct'] =  df_stock_data['mktval_shifted'] / df_stock_data[\"ttl_mktval\"]\n",
    "    ## drop columns\n",
    "    df_stock_data.drop(columns = {'shrout', 'mktval', 'mktval_shifted', 'ttl_mktval'}, inplace = True)\n",
    "    \n",
    "    return df_stock_data.sort_values(by=['date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb68d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ffm(date_start, date_end):\n",
    "    # start of time period \n",
    "    startdt = datetime.datetime(int(date_start[:4]),\n",
    "                                int(date_start[5:7]),\n",
    "                                int(date_start[8:]))\n",
    "    \n",
    "    # end of time period                            \n",
    "    enddt = datetime.datetime(int(date_end[:4]),\n",
    "                              int(date_end[5:7]),\n",
    "                              int(date_end[8:]))\n",
    "\n",
    "    # define which dataset are to be downloaded\n",
    "    d1 = web.DataReader('F-F_Research_Data_Factors_daily','famafrench',start=startdt, end=enddt)\n",
    "    d2 = web.DataReader('F-F_Momentum_Factor_daily','famafrench',start=startdt, end=enddt)\n",
    "\n",
    "    # key is 0 -> get returns data\n",
    "    # divide by 100 to get the returns\n",
    "    df_ff_3factor = d1[0]/100\n",
    "\n",
    "    # add momentum factor with an outer-join\n",
    "    # outer-join: keep all data -> union\n",
    "    df_ff_4factor = df_ff_3factor.join(d2[0]/100, how = 'outer')\n",
    "\n",
    "    # reset index for merge later\n",
    "    df_ff_4factor = df_ff_4factor.reset_index()\n",
    "\n",
    "    # change columns to be small letters and get rid of white-spaces\n",
    "    df_ff_4factor.columns = [z.lower().strip() for z in df_ff_4factor.columns]\n",
    "\n",
    "    # rename column\n",
    "    df_ff_4factor.rename(columns = {'mkt-rf':'mktrf'}, inplace = True)\n",
    "\n",
    "    # change order of dataframe\n",
    "    df_ff_4factor = df_ff_4factor.loc[:,['date', 'mktrf', 'smb', 'hml', 'mom', 'rf']]\n",
    "                              \n",
    "    return df_ff_4factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_excess_return(df, minuend, subtrahend):\n",
    "    # calculate excess return\n",
    "    df['excess_return'] = df[minuend]-df[subtrahend]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df_input, date_start, date_end, df_link, wrds_username, minuend, subtrahend):\n",
    "\n",
    "    ## retrieve data\n",
    "    df_stock_data = retrieve_stock_data(df_input=df_input_stocks, \n",
    "                                        date_start=date_start, \n",
    "                                        date_end=date_end, \n",
    "                                        wrds_username=wrds_username)\n",
    "    \n",
    "    # change type of entries in the columns date\n",
    "    df_link['date']  = pd.to_datetime(df_link['date'])\n",
    "    \n",
    "    ## merge with cusip\n",
    "    df_stock_data = pd.merge(df_stock_data, df_link.loc[:,['permno', 'date', 'secid']], how = \"left\", on = ['permno','date'])\n",
    "    \n",
    "    \n",
    "    # Download Fama-French Factors\n",
    "    df_ffm = download_ffm(date_start=date_start, date_end=date_end)\n",
    "    \n",
    "    ## merge stock data with ffm\n",
    "    # merge with ff-4-factors\n",
    "    df_stocks_ffm = pd.merge(df_stock_data, df_ffm, how='inner', on=['date'])\n",
    "    \n",
    "    # calculate excess return\n",
    "    df_stocks_ffm = calc_excess_return(df=df_stocks_ffm, minuend=minuend, subtrahend=subtrahend)\n",
    "    \n",
    "    return df_stocks_ffm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d275ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared = data_prep(df_input=df_input_stocks,\n",
    "                        date_start='1999-01-01',\n",
    "                        date_end='2020-12-31',\n",
    "                        df_link=df_link_permno_secid,\n",
    "                        wrds_username='strahinja23',\n",
    "                        minuend='ret',\n",
    "                        subtrahend='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09dd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_betas(df, window_size, period='daily'):\n",
    "    # identify all companies (permnos) in the dataframe\n",
    "    permnos = df.permno.unique()\n",
    "    # loop over all permnos and perform ols regression\n",
    "    for permno in tqdm(range(len(permnos))):\n",
    "        # define dataframe only with current permno\n",
    "        df_current_permno = df[df['permno'] == permnos[permno]]\n",
    "        # only perform OLS for stocks with more than 252 observations\n",
    "        if len(df_current_permno) > window_size:\n",
    "            # dependent variable\n",
    "            Y = df_current_permno['excess_return']\n",
    "            # independent variable\n",
    "            X = df_current_permno[['mktrf', 'smb', 'hml', 'mom']]\n",
    "            # define constant\n",
    "            X_constant = sm.add_constant(X)\n",
    "            # define model\n",
    "            rol_ols_model = RollingOLS(endog=Y, exog=X_constant, window=window_size)\n",
    "            # fitting\n",
    "            # print('Fitting rolling OLS model for permno #', permno, ' from', len(permnos))\n",
    "            results = rol_ols_model.fit()\n",
    "            # parameters\n",
    "            if permno == 0:\n",
    "                df_params = results.params\n",
    "            else:\n",
    "                df_params = pd.concat([df_params, results.params])\n",
    "\n",
    "    # rename columns\n",
    "    df_params.rename(columns = {'mktrf':'beta1', 'smb':'beta2', 'hml':'beta3', 'mom': 'beta4'}, inplace = True)\n",
    "    # merge with df_all by index\n",
    "    print('Merging the dataframes...')\n",
    "    df_betas = pd.merge(df, df_params, how='left', left_index=True, right_index=True, suffixes=[None, None])\n",
    "    # print df_ols\n",
    "    \n",
    "    if period == 'daily':\n",
    "        df_betas.dropna(inplace=True)\n",
    "        df_betas = df_betas.reset_index(drop=True)\n",
    "        return df_betas\n",
    "    \n",
    "    if period == 'monthly':\n",
    "        # create a temporary copy of df_ols\n",
    "        df_temp = df_betas[['permno', 'date', 'const', 'beta1', 'beta2', 'beta3', 'beta4']].copy()\n",
    "\n",
    "        # add necessary columns for group by\n",
    "        df_temp['year'] = df_temp['date'].dt.year\n",
    "        df_temp['month'] = df_temp['date'].dt.month\n",
    "        # group dataframe and select last row \n",
    "        df_temp = df_temp.groupby(['permno','year', 'month']).tail(n=1)\n",
    "        \n",
    "        # rename columns to indicate that those are the betas of the end of each month\n",
    "        df_temp.rename(columns = {'const':'const_eom', 'beta1':'beta1_eom', 'beta2':'beta2_eom', 'beta3':'beta3_eom', 'beta4':'beta4_eom'}, inplace = True)\n",
    "        # merge with df_ols\n",
    "        df_betas_monthly = pd.merge(df_betas, df_temp, how='left', on=['date', 'permno'])\n",
    "        # forward fill estimaed beta factors to assume that they stay constant for the next month until the last day\n",
    "        gb = df_betas_monthly.groupby('permno')\n",
    "        for var in [\"const_eom\", \"beta1_eom\", \"beta2_eom\", \"beta3_eom\", \"beta4_eom\"]:\n",
    "            df_betas_monthly[var] = gb[var].ffill()\n",
    "        \n",
    "        # drop daily betas\n",
    "        df_betas_monthly.drop(columns = {'const', 'beta1', 'beta2', 'beta3', 'beta4', 'year', 'month'}, inplace = True)\n",
    "\n",
    "        # drop nas\n",
    "        df_betas_monthly.dropna(inplace=True)\n",
    "\n",
    "        # reset index of df\n",
    "        df_betas_monthly = df_betas_monthly.reset_index(drop=True)\n",
    "        \n",
    "        return df_betas_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_betas = estimate_betas(df=df_prepared, window_size=252, period='monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_hdf(Path1,key='Homework2')\n",
    "df_features = df_features[['date','permno','iv','skew1','skew2','vrp']]\n",
    "df_betas = df_betas.merge(df_features,how='left',on=['date','permno'])\n",
    "df_betas.loc[:,'date'] = pd.to_datetime(df_betas.loc[:,'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d219a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_betas.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a846c53",
   "metadata": {},
   "source": [
    "## Code for Normalization and Rolling Window regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_sorted(df,date_of_split, dates):   \n",
    "    \n",
    "    date_of_split = pd.to_datetime(date_of_split)\n",
    "    X_train = df[(df[\"date\"] <= date_of_split) & (df['date'] > pd.datetime(1999, 12, 31))]\n",
    "    X_test = df[(df[\"date\"] > date_of_split) & (df['date'] < pd.datetime(2020, 1, 1))]\n",
    "    \n",
    "    X_train = X_train.sort_values(by=['date','permno'])\n",
    "    X_test = X_test.sort_values(by=['date','permno'])\n",
    "\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split_sorted(df_betas,pd.datetime(2012, 12, 31),df_betas.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def technicals_rock(df_a):\n",
    "    \n",
    "    KAMA = KAMAIndicator(close = df_a[\"prc\"],window = 10,pow1 = 2, pow2 = 30)\n",
    "    PPO = PercentagePriceOscillator(close = df_a[\"prc\"],window_slow = 26, window_fast = 12, window_sign = 9)\n",
    "    ROCI = ROCIndicator(close=df_a[\"prc\"],window=12)\n",
    "    RSI = RSIIndicator(close=df_a[\"prc\"],window = 14)\n",
    "    EMA14 = EMAIndicator(close = df_a[\"prc\"],window = 14)\n",
    "    EMA25 = EMAIndicator(close = df_a[\"prc\"],window = 25)\n",
    "    MACD_ind = MACD(close = df_a[\"prc\"],window_slow = 26,window_fast = 12,window_sign=9)\n",
    "    Aroon = AroonIndicator(close = df_a[\"prc\"],window = 25)\n",
    "    \n",
    "    df_a = df_a.assign(    \n",
    "    KAMA = KAMA.kama(),\n",
    "    PPO = PPO.ppo_signal(),\n",
    "    ROCI = ROCI.roc(),\n",
    "    RSI = RSI.rsi(),\n",
    "    EMA14 = EMA14.ema_indicator(),\n",
    "    EMA25 = EMA25.ema_indicator(),\n",
    "    MACD = MACD_ind.macd_signal(),\n",
    "    AroonInd = Aroon.aroon_indicator()\n",
    "    )\n",
    "    \n",
    "    return df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df,columns):\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    df_scaled = df_scaled.fillna(method='ffill',limit=30,inplace=False)\n",
    "    df_scaled = df_scaled.dropna()\n",
    "    \n",
    "    for column in columns:\n",
    "        df_scaled[column] = (df_scaled[column] - df_scaled[column].mean())  / df_scaled[column].std()\n",
    "        df_scaled.loc[:,column] = winsorize(df_scaled.loc[:,column], limits=0.03).data\n",
    "                                                                     \n",
    "    df_scaled.loc[:,'fret1d'] = df_scaled.groupby(['permno'])['ret'].shift(-1)\n",
    "    \n",
    "    df_scaled = df_scaled.dropna()\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2norm = ['skew1', 'skew2', 'iv','vrp','KAMA','PPO','RSI']\n",
    "\n",
    "train = process(technicals_rock(train),columns2norm)\n",
    "#test = process(technicals_rock(test),columns2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['skew1', 'skew2', 'iv','vrp','KAMA','PPO','RSI' ]#,'AroonInd']\n",
    "fig, axs = plt.subplots(3,3)\n",
    "fig.set_size_inches(18.5, 12.5, forward=True)\n",
    "fig.suptitle('Distribution of Variables', fontsize=18)\n",
    "\n",
    "for i,el in enumerate(cols):\n",
    "    a = train.hist(el, ax=axs.flatten()[i],bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec565ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rolling_Forest(data,predictors,target,window,params):\n",
    "\n",
    "# First we form the window \n",
    "    start_window = data.date.min()\n",
    "    end_window = data.date.min() + BDay(window)\n",
    "    pred_date = end_window + BDay(1)\n",
    "\n",
    "#Auxiliary placeholders and reg. constant\n",
    "    data['constant'] = 1.0\n",
    "    data = data.sort_values(by=['date','permno'])\n",
    "    predictions_df = pd.DataFrame(index=[pred_date], columns=np.sort(data.permno.unique()))\n",
    "  #  scores = []\n",
    "    missing_dates = []\n",
    "\n",
    "# The train / predict loop\n",
    "    while pred_date <= data.date.max():\n",
    "        try:\n",
    "            mask = ((data['date'] >= start_window) & (data['date'] <= end_window))\n",
    "            train_window = data[mask].set_index('permno')\n",
    "            linreg = RandomForestRegressor(**params)\n",
    "            fit_ = linreg.fit(train_window[predictors],train_window[target])\n",
    "            predi_df = data[data['date']== pred_date].loc[:,predictors+['permno']].set_index('permno')\n",
    "            predi = fit_.predict(predi_df)\n",
    "            one_predi = (pd.DataFrame(data = predi, index=predi_df.index,columns=[pred_date])).T\n",
    "            predictions_df = pd.concat([predictions_df,one_predi], axis=0, ignore_index=False)\n",
    "          #  score_r2 = linreg.score(train[predictors], train[target])\n",
    "          #  scores.append(score_r2)\n",
    "        except:\n",
    "            missing_dates.append(pred_date)\n",
    "# Update window position        \n",
    "        start_window = start_window + BDay(1)\n",
    "        end_window = start_window + BDay(window)\n",
    "        pred_date = end_window + BDay(1)\n",
    "# Real returns we will need\n",
    "\n",
    "    real_returns = data.pivot(index='date', columns='permno', values='fret1d')\n",
    "   # r2_average = np.mean(scores)\n",
    "    \n",
    "    return real_returns, predictions_df[1:] , missing_dates #, r2_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b395de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'n_estimators': [10, 20, 30],\n",
    "        'max_features': ['sqrt','auto','log2'],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_leaf':[1,2,3,4],\n",
    "        'min_samples_split':[2,3,4,5]\n",
    "        }\n",
    "\n",
    "regressors = ['skew1', 'skew2', 'iv','vrp']\n",
    "target = ['fret1d']\n",
    "\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "random_search = RandomizedSearchCV(forest, param_distributions=params, n_iter=param_comb, scoring='r2', \n",
    "                                   n_jobs=4, cv=KFold(5), verbose=3, random_state=1001 )\n",
    "                                                #KFold is not shuffled !\n",
    "\n",
    "# Here we go \n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(train[regressors], train[target])\n",
    "timer(start_time) # timing ends here for \"start_time\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_forest = {\n",
    "    \"n_estimators\": 30,\n",
    "    'max_features':'auto', \n",
    "    'max_depth':3,\n",
    "    'min_samples_leaf':4,\n",
    "    'min_samples_split':4,\n",
    "    'random_state':1001,\n",
    "    }\n",
    "\n",
    "real_returns, expected_returns,missing_dates = Rolling_Forest(train,regressors,target,21,params_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565db427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.pivot(index='date', columns='permno', values='fret1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888cedd",
   "metadata": {},
   "source": [
    "## Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ab835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_returns(real, expected, days_to_test):\n",
    "\n",
    "\n",
    "    real = real.loc[np.logical_and(real.index >= days_to_test - pd.Timedelta(days=252*3 + 1), real.index < days_to_test), :]\n",
    "    expected = expected.loc[np.logical_and(expected.index >= days_to_test - pd.Timedelta(days=252*3 + 1), expected.index < days_to_test), :]   \n",
    "    \n",
    "    real = real.fillna(0.0)\n",
    "    expected = expected.fillna(0.0)\n",
    "    \n",
    "    return real, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, expret = prepare_returns(real_returns,expected_returns,pd.datetime(2012, 12, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSR(wvec,*args):\n",
    "     cov = args[0]\n",
    "     mu  = args[1]  # Max Sharp Ratio\n",
    "     sr = mu@wvec/(wvec@cov@wvec)\n",
    "     return -sr / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29773b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Benchmark_Drawdown_Rolling(df,ret,d,lookback):\n",
    "    \n",
    "    d = pd.Timestamp(d)\n",
    "    start = d - BDay(lookback)\n",
    "    \n",
    "    means = ret.loc[start:(d-BDay(1)),:].values\n",
    "    \n",
    "    weights_bench = df[(df['date']>= start) & (df['date'] < d)]\n",
    "    weights_bench = weights_bench[['date','permno','weightage_pct']]\n",
    "    weights_bench = weights_bench.pivot(index = 'date',columns = 'permno',values='weightage_pct')\n",
    "    \n",
    "    monthly_benchmark_returns = (means*weights_bench).sum(axis=1)\n",
    "    monthly_benchmark_returns = pd.DataFrame(monthly_benchmark_returns,columns=[\"Benchmark_Return\"])\n",
    "    # cumulative return\n",
    "    monthly_benchmark_returns['Cum_ret'] = (1+ monthly_benchmark_returns['Benchmark_Return']).cumprod()\n",
    "    # cumulative peaks\n",
    "    monthly_benchmark_returns['Peaks'] = monthly_benchmark_returns['Cum_ret'].cummax()\n",
    "    # drawdown from trailing peak\n",
    "    monthly_benchmark_returns['Drawdown'] = (monthly_benchmark_returns['Cum_ret'] - monthly_benchmark_returns['Peaks']) / monthly_benchmark_returns['Peaks']\n",
    "    \n",
    "    return monthly_benchmark_returns, monthly_benchmark_returns['Drawdown'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817654c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_,_ = Benchmark_Drawdown_Rolling(train,ret,dates_rebal[21],21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.iloc[:,[1,2]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6872ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_.iloc[:,[3]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, max_dd = Benchmark_Drawdown_Rolling(train,ret,dates_rebal[21],21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_inputs_for_optimisation(df, ret, expret, d, limit,lookback):\n",
    "\n",
    "    cov = ret.cov().values \n",
    "    mu  = expret.iloc[-1].values \n",
    "    \n",
    "    if np.sum(mu > 0) == 0:\n",
    "        mu -= mu.mean()\n",
    "    \n",
    " # ----- Bounds on Weights Based on Benchmark on day d  ---- #\n",
    "    bounds = [ [] for _ in range(len(ret.columns)) ] # Empty placeholder for the 50 lower and upper bounds per day\n",
    "    for en,i in enumerate(ret.columns, start=0): # \n",
    "        \n",
    "        lower_bound = 0.9*float(df.loc[df['date']==d][df['permno']== i].weightage_pct.values)\n",
    "        higher_bound = 1.1*float(df.loc[df['date']==d][df['permno']== i].weightage_pct.values)\n",
    "        bounds[en].extend((lower_bound,higher_bound))\n",
    "              \n",
    " # ----- Constraints on Factor Exposure: ---- #\n",
    "    cons = []\n",
    "    fact = pd.DataFrame(columns=regressors+['permno'] )\n",
    "    fact.permno = ret.columns.tolist()\n",
    "    fact.set_index('permno', inplace=True)\n",
    "    fact = fact.fillna(0.0)\n",
    "    \n",
    "    for stock in fact.index:\n",
    "        for fct in fact.columns:\n",
    "            flt = np.logical_and(df['date'] == d, df['permno'] == stock)\n",
    "            if flt.any():\n",
    "                fact.loc[stock, fct] = df.loc[flt, fct].values[0]\n",
    "    \n",
    "    B = fact.T\n",
    "    \n",
    "    cons.append({'type': 'ineq', 'fun' : lambda wvec: limit - B@wvec})\n",
    "    cons.append({'type': 'ineq', 'fun' : lambda wvec: B@wvec + limit}) \n",
    " \n",
    " # ----- Constraints on Weight Sum (long portfolio): ---- #\n",
    "\n",
    "    cons.append({'type': 'eq', 'fun' : lambda wvec: wvec.sum()-1})\n",
    "\n",
    "#  # ----- Constraints on Drawdown: ---- #\n",
    "    if (pd.Timestamp(d)- BDay(lookback)) in ret.index:\n",
    "        _,benchmark_dd = Benchmark_Drawdown_Rolling(df,ret,d,21)\n",
    "        cons.append({'type': 'ineq', 'fun' : lambda wvec: mu.T@wvec-(benchmark_dd-0.01)})\n",
    "    \n",
    "    return cov, mu, cons, bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtall = pd.DataFrame(index = ret.index, data = ret.index, columns = ['dates']) #\n",
    "dt_m = dtall.groupby(pd.Grouper(freq='B')).last() # We can rebalane daily for final solution should be better results\n",
    "dates_rebal = dt_m.index.values                   # But less realistic (daily trading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a848a",
   "metadata": {},
   "source": [
    "## Sanity Check on Weight Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [ [] for _ in range(len(ret.columns)) ] # Empty placeholder for the 50 lower and upper bounds per day\n",
    "d = dates_rebal[0]\n",
    "for en,i in enumerate(ret.columns, start=0): # \n",
    "\n",
    "    lower_bound = 0.9*float(train.loc[train['date']==d][train['permno']== i].weightage_pct.values)\n",
    "    higher_bound = 1.1*float(train.loc[train['date']==d][train['permno']== i].weightage_pct.values)\n",
    "    bounds[en].extend((lower_bound,higher_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds[11] # Alles gut ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82535882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['date']==dates_rebal[0]][train['permno']== ret.columns[11]].weightage_pct.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a35bc",
   "metadata": {},
   "source": [
    "## Sanity Check on Factor Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = pd.DataFrame(columns=regressors+['permno'] )\n",
    "fact.permno = ret.columns.tolist()\n",
    "fact.set_index('permno', inplace=True)\n",
    "fact = fact.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in fact.index:\n",
    "    for fct in fact.columns:\n",
    "        flt = np.logical_and(train['date'] == dates_rebal[0], train['permno'] == stock)\n",
    "        if flt.any():\n",
    "            fact.loc[stock, fct] = train.loc[flt, fct].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d337e",
   "metadata": {},
   "source": [
    "## Run the whole loop on Weekly rebalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5011cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtall = pd.DataFrame(index = ret.index, data = ret.index, columns = ['dates']) #\n",
    "dt_m = dtall.groupby(pd.Grouper(freq='B')).last() # We can rebalane daily for final solution should be better results\n",
    "dates_rebal = dt_m.index.values                   # But less realistic (daily trading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e374411",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ret.columns\n",
    "ptf = {} \n",
    "\n",
    "# Initiate a dictionary of placeholders for the 3 strategies\n",
    "ptf['msr_w_cons'] = pd.DataFrame(data = 0.0, index = dates_rebal, columns = names)\n",
    "\n",
    "st_point_msr = None\n",
    "st_point_msr_all_names = train[['permno']].drop_duplicates()\n",
    "st_point_msr_all_names.insert(1, 'starting_point', 1.0/st_point_msr_all_names.shape[0])\n",
    "# Start from 1/N\n",
    "st_point_msr_all_names = st_point_msr_all_names.set_index('permno')\n",
    "print(st_point_msr_all_names.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022b7b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optimization rutine for the 2 minimization problems and for the closed form solution\n",
    "list_of_uneligible_dates = []\n",
    "for d in dates_rebal: \n",
    "    try:\n",
    "        print(d)    \n",
    "        start_time = time.time()\n",
    "\n",
    "        ret, expret = prepare_returns(real_returns,expected_returns,d)\n",
    "\n",
    "        # we prepare inputs:\n",
    "        cov,mu,cons,bounds = configure_inputs_for_optimisation(train, ret, expret, d, 0.05,21)\n",
    "\n",
    "        # define starting point\n",
    "        st_point_msr = st_point_msr_all_names.loc[st_point_msr_all_names.index.isin(ret.columns), 'starting_point'].T.values\n",
    "\n",
    "\n",
    "        # run optimisation, MSR\n",
    "        res = minimize(MSR,\n",
    "                    st_point_msr,\n",
    "                    args = (cov, mu), \n",
    "                    constraints = cons, \n",
    "                    bounds = bounds, \n",
    "                    method = 'SLSQP',\n",
    "                    options={'ftol': 1e-16, 'disp': False})\n",
    "        print(\"MSR: \" + res.message)\n",
    "        # store results\n",
    "        i = 0\n",
    "        for col in ret.columns:\n",
    "            ptf['msr_w_cons'].loc[d, col] = res.x[i]\n",
    "            i += 1\n",
    "\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    except:\n",
    "        list_of_uneligible_dates.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ret.columns[2:3]:\n",
    "    d= train.loc[train['date']==dates_rebal[16]][train['permno']== i].weightage_pct.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c921a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clc_str_return(w, ret, str_name):\n",
    "    w_i = np.zeros(len(ret.columns.tolist()))\n",
    "    ret_str = pd.DataFrame(index = ret.index, columns=[str_name])\n",
    "    for i in ret.index:\n",
    "        ret_str.loc[i, str_name] = w_i@ret.loc[i,:]\n",
    "        #ret_str.loc[i, str_name] = np.sum(w_i)\n",
    "        if i in w.index:\n",
    "            w_i = w.loc[i,:]\n",
    "#             if str_name == 'minvar_w':\n",
    "#                 negs = w_i.lt(0).sum().sum() # The heuristical solution in closed form\n",
    "#                 post = w_i.gt(0).sum().sum()\n",
    "#                 w_i[w_i>0] = w_i[w_i>0]/post\n",
    "#                 w_i[w_i<0] = w_i[w_i<0]/negs\n",
    "#                 delta_0 = w_i.sum()\n",
    "#                 if delta_0>0.01:\n",
    "#                     w_i=0\n",
    "#             if delta_0 > 0:\n",
    "#                 w_i[w_i<0] = w_i[w_i<0]*(1+delta_0)      \n",
    "#             else:\n",
    "#                 w_i[w_i>0] = w_i[w_i>0]*(1-delta_0)\n",
    "    \n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_str = clc_str_return(ptf['msr_w_cons'], ret, 'msr_w_cons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729666d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_ = qs.reports.html(ret_str['msr_w_cons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbdb2e",
   "metadata": {},
   "source": [
    "## Benchmark Seb's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the valued weighted average\n",
    "def weighted_avg(df, values, weights):\n",
    "    d = df[values]\n",
    "    w = df[weights]\n",
    "    return (d * w).sum() / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = (train.groupby(['date']).apply(weighted_avg, 'excess_return', 'weightage_pct'))\n",
    "df_plot = df_plot.loc[dates_rebal[0]:dates_rebal[-1]]\n",
    "df_plot.iloc[0] = df_plot.iloc[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot # Why is first value so high ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45915b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ret_str['msr_w_cons']*100 # Not sure why first 2 zeros ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b6045",
   "metadata": {},
   "source": [
    "## Final Report to Beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1289ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_ = qs.reports.html(ret_str['msr_w_cons'][2:],df_plot[1:]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
