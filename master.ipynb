{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Trading and Analysis with Python: Course Project\n",
    "\n",
    "Submitted by:\n",
    "\n",
    "Jiawei Li (XXX)  \n",
    "Sebastian Sydow (8316620)  \n",
    "Strahinja Trenkic (XXX)  \n",
    "Xiaoyi Zhou (XXX)  \n",
    "\n",
    "Date: 30th March 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we created several functions, which perform the necessary (sub-)steps for the given task. This allowed us to easily reproduce results or change certain parameters. Please refer to the docstrings at the beginning of and the comments within the functions for further explanations. \n",
    "\n",
    "**Note:** Some of the explanations or quality checks in this notebook might seem too exhaustive or dispensable for the given task. Nevertheless, these explanations and quality checks were added as they facilitated in the process of understanding the concepts and shall serve as a future point of reference for us. Moreover, this notebook used some of the code provided during the lectures of the course *Quantitative Trading and Analysis with Python* (M.Sc.) at the Frankfurt School of Finance and Management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib.axis import Axis\n",
    "from matplotlib.pyplot import figure\n",
    "from pandas.tseries.offsets import BDay\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from ta.momentum import KAMAIndicator\n",
    "from ta.momentum import PercentagePriceOscillator\n",
    "from ta.momentum import ROCIndicator\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import AroonIndicator\n",
    "from ta.trend import EMAIndicator\n",
    "from ta.trend import MACD\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader as web\n",
    "import quandl as quandl\n",
    "import quantstats as qs\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import ta as ta\n",
    "import wrds as wrds\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperations\n",
    "\n",
    "In the next cell, the data of different data sources is loaded and preprocessed. The data for the generalized lower bounds for the expected excess simple returns (`df_glb`) and  risk-neutral skewness (`df_mfis`) were provided by [Grigory Vilkov](https://www.vilkov.net/index.html) and retrieved from [here](https://osf.io/z2486/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRDS username\n",
    "wrds_username = \"iewaij\"\n",
    "# define the path to data folder\n",
    "path = Path(\"./data\")\n",
    "# stocks\n",
    "input_stocks = path / \"permno_selection.csv\"\n",
    "# link betweeen CRSP (permno) and Option Metrics (SECID)\n",
    "link_permno_secid = path / \"daily_permno_secid_cusip_link.csv.zip\"\n",
    "# glb\n",
    "glb = path / \"glb_daily.csv\"\n",
    "# model-free implied skewness (MFIS)\n",
    "mfis = path / \"MFIS_1996_2019.csv\"\n",
    "# Save a hdf locally for easier modeling\n",
    "hdf_path = path / \"factors.h5\"\n",
    "# quandl key\n",
    "quandl_key = \"XWVLTpJSLyiA8s66Fh8x\"\n",
    "# import stock universe\n",
    "df_input_stocks = pd.read_csv(input_stocks)\n",
    "# glb\n",
    "df_glb = pd.read_csv(glb)\n",
    "# rename column\n",
    "df_glb = df_glb.rename(columns={\"id\": \"permno\"})\n",
    "# transfrom data type\n",
    "df_glb[\"date\"] = pd.to_datetime(df_glb[\"date\"])\n",
    "# mfis\n",
    "df_mfis = pd.read_csv(mfis)\n",
    "# rename column\n",
    "df_mfis = df_mfis.rename(columns={\"id\": \"permno\"})\n",
    "# transfrom data type\n",
    "df_mfis[\"date\"] = pd.to_datetime(df_mfis[\"date\"])\n",
    "# instantiate zip-file\n",
    "zip_file = ZipFile(link_permno_secid)\n",
    "# load linking table\n",
    "df_link_permno_secid = pd.read_csv(zip_file.open(\"daily_permno_secid_cusip_link.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Returns and Market Capitalization\n",
    "In the next cell, we created a function to retrieve the daily stock returns, daily stock prices and daily shares outstanding for the companies in `df_input_stocks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_stock_data(df_input, date_start, date_end, wrds_username):\n",
    "    \"\"\"\n",
    "    This function retrieves the permno, cusip return, price, shares outstanding and market value of each stock\n",
    "    on each day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input: dataframe\n",
    "        The dataframe storing the information about the stocks.\n",
    "        \n",
    "    date_start: str\n",
    "        First date of data retrieval.\n",
    "        \n",
    "    date_end: str\n",
    "        Last date of data retrieval.\n",
    "    \n",
    "    wrds_username: str\n",
    "        Username of the wrds-account used for the data retrieval.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the information for each stock on each day \n",
    "    as well as the weightage of each stock on each day by market value in a dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Download stock data\n",
    "    ## establish WRDS connection\n",
    "    db = wrds.Connection(wrds_username=wrds_username)\n",
    "    # create query to load the returns, prices and shares outstanding for S&P 500 companies from 1999/01\n",
    "    sql_wrds = \"\"\"\n",
    "            select distinct date, \n",
    "                            permno,\n",
    "                            cusip, \n",
    "                            ret, \n",
    "                            abs(prc) as prc, \n",
    "                            shrout,\n",
    "                            abs(prc)*shrout/1000 as mktval\n",
    "            from crsp.dsf \n",
    "            where permno in %(permno)s and date>=%(start)s and date<=%(end)s\n",
    "            order by date\n",
    "            \"\"\"\n",
    "\n",
    "    # define the parameters\n",
    "    params = {}\n",
    "    params['start'] = date_start\n",
    "    params['end'] = date_end\n",
    "    params['permno'] = tuple(df_input.permno.unique().astype(str))\n",
    "\n",
    "    # retrieve the data from wrds\n",
    "    df_stock_data = db.raw_sql(sql_wrds, params = params)\n",
    "\n",
    "    # change type of entries in the columns start and ending\n",
    "    df_stock_data['date']  = pd.to_datetime(df_stock_data['date'])\n",
    "    \n",
    "    # shift market_val by 1 day\n",
    "    df_stock_data.loc[:,'mktval_shifted'] = df_stock_data.groupby(['permno'])['mktval'].shift(1)\n",
    "    \n",
    "    # calculate weight of each stock on each day\n",
    "    ## dataframe with total market value on each day\n",
    "    df_ttl_mkcap = df_stock_data[['date', 'mktval_shifted']].groupby('date').sum().rename(columns={\"mktval_shifted\": \"ttl_mktval\"})\n",
    "    ## merge with df_stock_data\n",
    "    df_stock_data = pd.merge(df_stock_data, df_ttl_mkcap, how='left', on=['date'])\n",
    "    ## calculate weightage\n",
    "    df_stock_data['weightage_pct'] =  df_stock_data['mktval_shifted'] / df_stock_data[\"ttl_mktval\"]\n",
    "    ## drop columns\n",
    "    df_stock_data.drop(columns = {'shrout', 'mktval', 'mktval_shifted', 'ttl_mktval'}, inplace = True) \n",
    "    db.close()\n",
    "    return df_stock_data.sort_values(by=['date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fama-French Factors\n",
    "In the next cell, we created a function to download the data for the four-factor model (daily) from the Ken French data library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ffm(date_start, date_end):\n",
    "    \"\"\"\n",
    "    This function download the data for the four-factor model (daily) from the Ken French data library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date_start: str\n",
    "        First date of data retrieval.\n",
    "        \n",
    "    date_end: str\n",
    "        Last date of data retrieval.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the data for the four-factor model (daily) from the Ken French data library\n",
    "    in a dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    # start of time period \n",
    "    startdt = datetime.datetime(int(date_start[:4]),\n",
    "                                int(date_start[5:7]),\n",
    "                                int(date_start[8:]))\n",
    "    \n",
    "    # end of time period                            \n",
    "    enddt = datetime.datetime(int(date_end[:4]),\n",
    "                              int(date_end[5:7]),\n",
    "                              int(date_end[8:]))\n",
    "\n",
    "    # define which dataset are to be downloaded\n",
    "    d1 = web.DataReader('F-F_Research_Data_Factors_daily','famafrench',start=startdt, end=enddt)\n",
    "    d2 = web.DataReader('F-F_Momentum_Factor_daily','famafrench',start=startdt, end=enddt)\n",
    "\n",
    "    # key is 0 -> get returns data\n",
    "    # divide by 100 to get the returns\n",
    "    df_ff_3factor = d1[0]/100\n",
    "\n",
    "    # add momentum factor with an outer-join\n",
    "    # outer-join: keep all data -> union\n",
    "    df_ff_4factor = df_ff_3factor.join(d2[0]/100, how = 'outer')\n",
    "\n",
    "    # reset index for merge later\n",
    "    df_ff_4factor = df_ff_4factor.reset_index()\n",
    "\n",
    "    # change columns to be small letters and get rid of white-spaces\n",
    "    df_ff_4factor.columns = [z.lower().strip() for z in df_ff_4factor.columns]\n",
    "\n",
    "    # rename column\n",
    "    df_ff_4factor.rename(columns = {'mkt-rf':'mktrf'}, inplace = True)\n",
    "\n",
    "    # change order of dataframe\n",
    "    df_ff_4factor = df_ff_4factor.loc[:,['date', 'mktrf', 'smb', 'hml', 'mom', 'rf']]\n",
    "                              \n",
    "    return df_ff_4factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quandl Data\n",
    "\n",
    "In the next cell, we created a function to download data from quandl, among others:\n",
    "\n",
    "**Crude Oil Prices**\n",
    "- [WTI Spot Price](https://data.nasdaq.com/data/EIA/PET_RWTC_D-cushing-ok-wti-spot-price-fob-daily)\n",
    "\n",
    "**Treasury Yields**\n",
    "- [US Treasury Yields](https://data.nasdaq.com/data/USTREASURY/YIELD-treasury-yield-curve-rates)\n",
    "\n",
    "**Gold**\n",
    "- [LBMA/GOLD](https://data.nasdaq.com/data/LBMA/GOLD-gold-price-london-fixing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quandl_data(quandl_key):\n",
    "    \"\"\"\n",
    "    This function download the data from quandl, which can later be utilized to construct factors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    quandl_key: path\n",
    "        Path to an excel-file, which stores the quandl-key in the first cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns three dataframes: \n",
    "    - Daily WTI Spot Prices\n",
    "    - Daily yield difference between 10year yield and 1year yield of US treasuries\n",
    "    - Daily price of gold\n",
    "    \"\"\"\n",
    "    \n",
    "    # configure quandl\n",
    "    quandl.ApiConfig.api_key = quandl_key\n",
    "    \n",
    "    # Crude Oil\n",
    "    df_crude_oil = quandl.get(\"EIA/PET_RWTC_D\")\n",
    "    # calculate daily returns\n",
    "    df_crude_oil = df_crude_oil.pct_change(1)\n",
    "    # reset_index\n",
    "    df_crude_oil.reset_index(inplace=True)\n",
    "    # rename column\n",
    "    df_crude_oil.rename(columns={\"Value\": \"ret_crude_oil\", \"Date\": \"date\"}, inplace=True)\n",
    "    \n",
    "    # Treasury Yields\n",
    "    df_treasury_yield = quandl.get(\"USTREASURY/YIELD\")\n",
    "    # drop columns\n",
    "    df_treasury_yield.drop(labels=['1 MO', '2 MO', '3 MO', '6 MO', '2 YR', '3 YR', '5 YR', '7 YR', '20 YR', '30 YR'], axis=1, inplace=True)\n",
    "    # calculate difference between 10year yield and 1year yield\n",
    "    df_treasury_yield['yield_dif_10y_1y'] = df_treasury_yield['10 YR'] - df_treasury_yield['1 YR']\n",
    "    # drop columns\n",
    "    df_treasury_yield.drop(labels=['1 YR', '10 YR'], axis=1, inplace=True)\n",
    "    # reset_index\n",
    "    df_treasury_yield.reset_index(inplace=True)\n",
    "    # rename column\n",
    "    df_treasury_yield.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "    \n",
    "    # Gold\n",
    "    df_gold = quandl.get(\"LBMA/GOLD\")\n",
    "    # drop columns\n",
    "    df_gold.drop(labels=['USD (PM)','GBP (AM)', 'GBP (PM)', 'EURO (AM)', 'EURO (PM)'], axis=1, inplace=True)\n",
    "    # calculate daily returns\n",
    "    df_gold = df_gold.pct_change(1)\n",
    "    # reset_index\n",
    "    df_gold.reset_index(inplace=True)\n",
    "    # rename column\n",
    "    df_gold.rename(columns={\"USD (AM)\": \"ret_gold\", \"Date\": \"date\"}, inplace=True)\n",
    "    \n",
    "    return df_crude_oil, df_treasury_yield, df_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exchange Rate\n",
    "\n",
    "Data about exchange rates were retrieved from the St. Louis FED with the function `get_fx_data`.\n",
    "- [Exchange Rates ST. Louis FRED](https://fred.stlouisfed.org/categories/15)\n",
    "- [Nominal Emerging Market Economies U.S. Dollar Index](https://fred.stlouisfed.org/series/DTWEXEMEGS), only available from 2006 onwards\n",
    "- [Real Emerging Market Economies Dollar Index](https://fred.stlouisfed.org/series/RTWEXEMEGS), only available from 2006 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fx_data(symbol, start_date):\n",
    "    \"\"\"\n",
    "    This function downloads the data about exchange rates from the ST. Louis FED.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    symbol: symbol\n",
    "        Identifier of the data provided by the St. Louis FED.\n",
    "        \n",
    "    start_date: datetime\n",
    "        First date of data retrieval.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the dataframe with the daily returns of the forex data. \n",
    "    \"\"\"\n",
    "    # get data\n",
    "    df_fx = web.DataReader(symbol, 'fred', start_date)\n",
    "    # calculate daily returns\n",
    "    df_fx = df_fx.pct_change(1)\n",
    "    # reset index\n",
    "    df_fx.reset_index(inplace=True)\n",
    "    # rename\n",
    "    df_fx.rename(columns={'DATE': 'date', symbol: 'fx_rates'}, inplace=True)\n",
    "    \n",
    "    return df_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Excess Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_excess_return(df, minuend, subtrahend):\n",
    "    \"\"\"\n",
    "    This function calculates the excess return.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: dataframe\n",
    "        Identifier of the data provided by the St. Louis FED.\n",
    "\n",
    "    minuend: str\n",
    "        Column storing data about the minuend.\n",
    "\n",
    "    subtrahend: str\n",
    "        Column storing data about the subtrahend.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the dataframe with an additional column for the excess return.\n",
    "    \"\"\"\n",
    "    # calculate excess return\n",
    "    df[\"excess_return\"] = df[minuend] - df[subtrahend]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfrom Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(\n",
    "    df_input,\n",
    "    date_start,\n",
    "    date_end,\n",
    "    df_link,\n",
    "    wrds_username,\n",
    "    minuend,\n",
    "    subtrahend,\n",
    "    quandl_key,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function calls the previously created functions to load and pre-process the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input: dataframe\n",
    "        The dataframe storing the information about the stocks.\n",
    "\n",
    "    date_start: str\n",
    "        First date of data retrieval.\n",
    "\n",
    "    date_end: str\n",
    "        Last date of data retrieval.\n",
    "\n",
    "    df_link: dataframe\n",
    "        The dataframe storing the mapping between CRSP [permno] and OptionMetrics [secid] identifiers.\n",
    "\n",
    "    wrds_username: str\n",
    "        Username of the wrds-account used for the data retrieval.\n",
    "\n",
    "    minuend: str\n",
    "        Column storing data about the minuend.\n",
    "\n",
    "    subtrahend: str\n",
    "        Column storing data about the subtrahend.\n",
    "\n",
    "    quandl_key: path\n",
    "        Path to an excel-file, which stores the quandl-key in the first cell.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the downloaded and pre-processed data as a dataframe.\n",
    "    \"\"\"\n",
    "    # retrieve data\n",
    "    df_stock_data = retrieve_stock_data(\n",
    "        df_input=df_input,\n",
    "        date_start=date_start,\n",
    "        date_end=date_end,\n",
    "        wrds_username=wrds_username,\n",
    "    )\n",
    "\n",
    "    # change type of entries in the columns date\n",
    "    df_link[\"date\"] = pd.to_datetime(df_link[\"date\"])\n",
    "    # merge with cusip\n",
    "    df_stock_data = pd.merge(\n",
    "        df_stock_data,\n",
    "        df_link.loc[:, [\"permno\", \"date\", \"secid\"]],\n",
    "        how=\"left\",\n",
    "        on=[\"permno\", \"date\"],\n",
    "    )\n",
    "    # Download Fama-French Factors\n",
    "    df_ffm = download_ffm(date_start=date_start, date_end=date_end)\n",
    "    # merge stock data with ffm\n",
    "    df_stocks_factors = pd.merge(df_stock_data, df_ffm, how=\"inner\", on=[\"date\"])\n",
    "    # calculate excess return\n",
    "    df_stocks_factors = calc_excess_return(\n",
    "        df=df_stocks_factors, minuend=minuend, subtrahend=subtrahend\n",
    "    )\n",
    "    # get quandl data\n",
    "    df_crude, df_yields, df_gold = get_quandl_data(quandl_key=quandl_key)\n",
    "    # merge with stock data\n",
    "    df_stocks_factors = pd.merge(df_stocks_factors, df_crude, how=\"left\", on=[\"date\"])\n",
    "    df_stocks_factors = pd.merge(df_stocks_factors, df_yields, how=\"left\", on=[\"date\"])\n",
    "    df_stocks_factors = pd.merge(df_stocks_factors, df_gold, how=\"left\", on=[\"date\"])\n",
    "\n",
    "    return df_stocks_factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we call the function `data_prep` to download, pre-process and merge the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared = data_prep(\n",
    "    df_input=df_input_stocks,\n",
    "    date_start=\"1999-01-01\",\n",
    "    date_end=\"2019-12-31\",\n",
    "    df_link=df_link_permno_secid,\n",
    "    wrds_username=wrds_username,\n",
    "    minuend=\"ret\",\n",
    "    subtrahend=\"rf\",\n",
    "    quandl_key=quandl_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of $\\beta$-Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_betas(df, window_size, factors, period=\"daily\"):\n",
    "    \"\"\"\n",
    "    This function calls the previously estimates the beta-factors for those characteristics,\n",
    "    which are not directly observable for each stock.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input: dataframe\n",
    "        The dataframe storing the information about the stocks.\n",
    "\n",
    "    windo_size: int\n",
    "        The window for whcich the betas shall be estimated.\n",
    "\n",
    "    factors: list\n",
    "        List of factors, for which betas need to be estimated.\n",
    "\n",
    "    period: str\n",
    "        The period of recalculating the betas, (daily or monthly).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns dataframe with etimated betas, which are unique for each stock.\n",
    "    \"\"\"\n",
    "    # identify all companies (permnos) in the dataframe\n",
    "    permnos = df.permno.unique()\n",
    "    # loop over all permnos and perform ols regression\n",
    "    for permno in tqdm(range(len(permnos))):\n",
    "        # define dataframe only with current permno\n",
    "        df_current_permno = df[df[\"permno\"] == permnos[permno]]\n",
    "        # only perform OLS for stocks with more than 252 observations\n",
    "        if len(df_current_permno) > window_size:\n",
    "            # dependent variable\n",
    "            Y = df_current_permno[\"excess_return\"]\n",
    "            # independent variable\n",
    "            X = df_current_permno[factors]\n",
    "            # define constant\n",
    "            X_constant = sm.add_constant(X)\n",
    "            # define model\n",
    "            rol_ols_model = RollingOLS(endog=Y, exog=X_constant, window=window_size)\n",
    "            # fitting\n",
    "            # print('Fitting rolling OLS model for permno #', permno, ' from', len(permnos))\n",
    "            results = rol_ols_model.fit()\n",
    "            # parameters\n",
    "            if permno == 0:\n",
    "                df_params = results.params\n",
    "            else:\n",
    "                df_params = pd.concat([df_params, results.params])\n",
    "\n",
    "    # rename columns\n",
    "    for col in factors:\n",
    "        df_params.rename(columns={col: col + str(\"_beta\")}, inplace=True)\n",
    "    # df_params.rename(columns = {'mktrf':'beta1', 'smb':'beta2', 'hml':'beta3', 'mom': 'beta4'}, inplace = True)\n",
    "    # merge with df_all by index\n",
    "    print(\"Merging the dataframes...\")\n",
    "    df_betas = pd.merge(\n",
    "        df,\n",
    "        df_params,\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        suffixes=[None, None],\n",
    "    )\n",
    "    print(\"Merging completed.\")\n",
    "\n",
    "    if period == \"daily\":\n",
    "        df_betas.dropna(inplace=True)\n",
    "        df_betas = df_betas.reset_index(drop=True)\n",
    "        return df_betas\n",
    "\n",
    "    if period == \"monthly\":\n",
    "        # add '_factor' to the entries in list factors\n",
    "        ls_factors_betas = [\n",
    "            factors[i] + str(\"_beta\") for i in list(range(len(factors)))\n",
    "        ]\n",
    "        # create a temporary copy of df_betas\n",
    "        cols2copy = [\"permno\", \"date\", \"const\"] + ls_factors_betas\n",
    "        df_temp = df_betas[cols2copy].copy()\n",
    "        # df_temp = df_betas[['permno', 'date', 'const', 'beta1', 'beta2', 'beta3', 'beta4']].copy()\n",
    "\n",
    "        # add necessary columns for group by\n",
    "        df_temp[\"year\"] = df_temp[\"date\"].dt.year\n",
    "        df_temp[\"month\"] = df_temp[\"date\"].dt.month\n",
    "        # group dataframe and select last row\n",
    "        df_temp = df_temp.groupby([\"permno\", \"year\", \"month\"]).tail(n=1)\n",
    "\n",
    "        # rename columns to indicate that those are the betas of the end of each month\n",
    "        for col in cols2copy[2:]:\n",
    "            df_temp.rename(columns={col: col + str(\"_eom\")}, inplace=True)\n",
    "        # df_temp.rename(columns = {'const':'const_eom', 'beta1':'beta1_eom', 'beta2':'beta2_eom', 'beta3':'beta3_eom', 'beta4':'beta4_eom'}, inplace = True)\n",
    "        # merge with df_betas\n",
    "        df_betas_monthly = pd.merge(\n",
    "            df_betas, df_temp, how=\"left\", on=[\"date\", \"permno\"]\n",
    "        )\n",
    "        # forward fill estimaed beta factors to assume that they stay constant for the next month until the last day\n",
    "        gb = df_betas_monthly.groupby(\"permno\")\n",
    "        cols_eom = cols2copy[2:]\n",
    "        cols_eom = [cols_eom[i] + str(\"_eom\") for i in list(range(len(cols_eom)))]\n",
    "        for var in cols_eom:\n",
    "            df_betas_monthly[var] = gb[var].ffill()\n",
    "\n",
    "        # drop daily betas\n",
    "        # df_betas_monthly.drop(columns = {'const', 'beta1', 'beta2', 'beta3', 'beta4', 'year', 'month'}, inplace = True)\n",
    "        df_betas_monthly.drop(\n",
    "            [\"const\", \"year\", \"month\"] + ls_factors_betas + factors,\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "        # drop nas\n",
    "        df_betas_monthly.dropna(inplace=True)\n",
    "\n",
    "        # reset index of df\n",
    "        df_betas_monthly = df_betas_monthly.reset_index(drop=True)\n",
    "\n",
    "        return df_betas_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [\n",
    "    \"mktrf\",\n",
    "    \"smb\",\n",
    "    \"hml\",\n",
    "    \"mom\",\n",
    "    \"ret_crude_oil\",\n",
    "    \"yield_dif_10y_1y\",\n",
    "    \"ret_gold\",\n",
    "]\n",
    "\n",
    "df_betas = estimate_betas(\n",
    "    df=df_prepared, window_size=252, factors=factors, period=\"monthly\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Benchmark Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg(df, values, weights):\n",
    "    \"\"\"\n",
    "    Calculate the valued weighted average.\n",
    "    \"\"\"\n",
    "    d = df[values]\n",
    "    w = df[weights]\n",
    "    return (d * w).sum() / w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_benchmark_performance(df, column_ret, column_weight):\n",
    "    # calculate benchmark performance on each day\n",
    "    df_plot = df.groupby([\"date\"]).apply(weighted_avg, column_ret, column_weight) + 1\n",
    "    df_plot.iloc[0] = df_plot.iloc[0] * 100\n",
    "\n",
    "    # size of the plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "    # define the spacing on the x-axis\n",
    "    ## major ticks every 4 years\n",
    "    major_ticks_years = mdates.YearLocator(4)\n",
    "    Axis.set_major_locator(ax.xaxis, major_ticks_years)\n",
    "    ## Minor ticks every year\n",
    "    minor_ticks_year = mdates.YearLocator(1)\n",
    "    Axis.set_minor_locator(ax.xaxis, minor_ticks_year)\n",
    "\n",
    "    # title\n",
    "    plt.title(r\"Value-Weighted Excess-Return of the Benchmark-Portfolio\")\n",
    "\n",
    "    # format the date type to year on the x-axis\n",
    "    ax.format_xdata = mdates.DateFormatter(\"% m\")\n",
    "\n",
    "    # 9/11\n",
    "    ax.annotate(\n",
    "        \"9/11\",\n",
    "        xy=(\"2001-09-11 00:00:00\", 120),\n",
    "        xytext=(15, 15),\n",
    "        textcoords=\"offset points\",\n",
    "        arrowprops=dict(arrowstyle=\"-|>\"),\n",
    "    )\n",
    "    ax.axvline(x=pd.to_datetime(\"2001-09-11\"), color=\"r\", linestyle=\"--\", lw=1)\n",
    "\n",
    "    # bankruptcy Lehman\n",
    "    ax.annotate(\n",
    "        \"Bankruptcy Lehman Brothers\",\n",
    "        xy=(\"2008-09-15 00:00:00\", 90),\n",
    "        xytext=(15, 15),\n",
    "        textcoords=\"offset points\",\n",
    "        arrowprops=dict(arrowstyle=\"-|>\"),\n",
    "    )\n",
    "    ax.axvline(x=pd.to_datetime(\"2008-09-15\"), color=\"r\", linestyle=\"--\", lw=1)\n",
    "\n",
    "    # covid\n",
    "    ax.annotate(\n",
    "        \"Covid-19\",\n",
    "        xy=(\"2020-03-16 00:00:00\", 350),\n",
    "        xytext=(-60, 5),\n",
    "        textcoords=\"offset points\",\n",
    "        arrowprops=dict(arrowstyle=\"-|>\"),\n",
    "    )\n",
    "    ax.axvline(x=pd.to_datetime(\"2020-03-16\"), color=\"r\", linestyle=\"--\", lw=1)\n",
    "\n",
    "    # define start and end of x-axis\n",
    "    ax.set_xlim(min(df[\"date\"]), max(df[\"date\"]))\n",
    "\n",
    "    # define labels of x- and y-axis\n",
    "    ax.set(\n",
    "        xlabel=\"Date\",\n",
    "        ylabel=\"Performance in % (Value-Weighted Mean of Benchmark Portfolio)\",\n",
    "    )\n",
    "\n",
    "    ax.plot(df_plot.cumprod(), label=\"Excess Return Benchmark\")\n",
    "    ax.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_benchmark_performance(df=df_betas, column_ret=\"ret\", column_weight=\"weightage_pct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option-Implied Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLB and MFIS\n",
    "\n",
    "In the next cell, the data for the generalized lower bounds for the expected excess simple returns (`df_glb`) and  risk-neutral skewness (`df_mfis`) are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data with glb\n",
    "df_stocks_betas_options = pd.merge(df_betas, df_glb, how=\"left\", on=['permno', 'date'])\n",
    "# merge data with mfis\n",
    "df_stocks_betas_options = pd.merge(df_stocks_betas_options, df_mfis, how='left', on=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Volatility and SKEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_option_metrics(\n",
    "    df,\n",
    "    start_year,\n",
    "    end_year,\n",
    "    days,\n",
    "    delta_otm_put,\n",
    "    delta_atm_call,\n",
    "    delta_otm_put2,\n",
    "    delta_atm_call2,\n",
    "    wrds_username,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    This function retrieves the option metrics from WRDS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: dataframe\n",
    "        The dataframe storing the information about the stocks.\n",
    "\n",
    "    start_year: int\n",
    "        First year of data retrieval.\n",
    "\n",
    "    end_year: int\n",
    "        Last year of data retrieval.\n",
    "\n",
    "    days: int\n",
    "        Number of days used for calulating the metrics.\n",
    "\n",
    "    delta_otm_put: int\n",
    "        Used for calculating skew, OTM put (delta)\n",
    "\n",
    "    delta_atm_call: int\n",
    "        Used for calculating skew, ATM put (delta)\n",
    "\n",
    "    delta_otm_put2: int\n",
    "        Used for calculating skew2, OTM put (delta)\n",
    "\n",
    "    delta_atm_call2: int\n",
    "        Used for calculating skew2, ATM put (delta)\n",
    "\n",
    "    wrds_username: str\n",
    "        Username of the wrds-account used for the data retrieval.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the downloaded option metrics as a dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ## establish WRDS connection\n",
    "    db = wrds.Connection(wrds_username=wrds_username)\n",
    "\n",
    "    # define the parameters\n",
    "    params = {}\n",
    "    params[\"secids\"] = tuple(df.secid.unique())\n",
    "    params[\"days\"] = days\n",
    "    params[\"delta_otm_put\"] = delta_otm_put\n",
    "    params[\"delta_atm_call\"] = delta_atm_call\n",
    "    params[\"delta_otm_put2\"] = delta_otm_put2\n",
    "    params[\"delta_atm_call2\"] = delta_atm_call2\n",
    "\n",
    "    # define the sql query\n",
    "    sql = \"\"\"\n",
    "    select a.date, a.secid, a.iv-b.iv as skew1, c.iv, (g.iv-v.iv)/a.iv as skew2 from \n",
    "\n",
    "    (select date, secid, impl_volatility as iv \n",
    "    from optionm.vsurfd%(year)s\n",
    "    where secid in %(secids)s and days = %(days)s and delta=%(delta_atm_call)s) as a, \n",
    "\n",
    "    (select date, secid, impl_volatility as iv \n",
    "    from optionm.vsurfd%(year)s\n",
    "    where secid in %(secids)s and days = %(days)s and delta=%(delta_otm_put)s) as b, \n",
    "\n",
    "    (select date, secid, AVG(impl_volatility) as iv \n",
    "    from optionm.vsurfd%(year)s\n",
    "    where secid in %(secids)s and days = %(days)s and abs(delta)<=%(delta_atm_call)s\n",
    "    group by date, secid) as c,\n",
    "    \n",
    "    (select date, secid, impl_volatility as iv \n",
    "    from optionm.vsurfd%(year)s\n",
    "    where secid in %(secids)s and days = %(days)s and delta=%(delta_otm_put2)s) as g, \n",
    "\n",
    "    (select date, secid, impl_volatility as iv \n",
    "    from optionm.vsurfd%(year)s\n",
    "    where secid in %(secids)s and days = %(days)s and delta=%(delta_atm_call2)s) as v\n",
    "\n",
    "    where a.date= b.date and a.secid = b.secid \n",
    "    and a.date = c.date and a.secid = c.secid\n",
    "    and a.date = g.date and a.secid = g.secid\n",
    "    and a.date = v.date and a.secid = v.secid\n",
    "    \"\"\"\n",
    "    # retrieve data for 1999\n",
    "    params[\"year\"] = start_year\n",
    "    df_optiondata = db.raw_sql(sql, params=params)\n",
    "\n",
    "    # append data for each year from 2000 to 2020\n",
    "    for y in tqdm(range(start_year + 1, end_year)):\n",
    "        # print('Processing year ', y)\n",
    "        # set parameter for the year\n",
    "        params[\"year\"] = y\n",
    "        df_optiondata = pd.concat([df_optiondata, db.raw_sql(sql, params=params)])\n",
    "    db.close()\n",
    "    return df_optiondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_option_metrics = retrieve_option_metrics(\n",
    "    df=df_stocks_betas_options,\n",
    "    start_year=1999,\n",
    "    end_year=2020,\n",
    "    days=30,\n",
    "    delta_otm_put=-10,\n",
    "    delta_atm_call=50,\n",
    "    delta_otm_put2=-25,\n",
    "    delta_atm_call2=25,\n",
    "    wrds_username=wrds_username,\n",
    ")\n",
    "\n",
    "# change type of entries in the column data\n",
    "df_option_metrics.loc[:, \"date\"] = pd.to_datetime(df_option_metrics.loc[:, \"date\"])\n",
    "\n",
    "# merge optiondata and stockdata\n",
    "df_stocks_betas_options = pd.merge(\n",
    "    df_stocks_betas_options, df_option_metrics, how=\"left\", on=[\"secid\", \"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Risk Premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vrp(wrds_username):\n",
    "    db = wrds.Connection(wrds_username=wrds_username)\n",
    "\n",
    "    sql = \"\"\"\n",
    "    select a.date,a.secid,a.return\n",
    "    from optionm.secprd%(year)s as a\n",
    "    where secid in %(secids)s\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    params[\"secids\"] = tuple(df_stocks_betas_options.secid.unique())\n",
    "    params[\"year\"] = 1999\n",
    "    options_returns = db.raw_sql(sql, params=params)\n",
    "\n",
    "    for y in tqdm(range(2000, 2020)):\n",
    "        params[\"year\"] = y\n",
    "        options_returns = pd.concat([options_returns, db.raw_sql(sql, params=params)])\n",
    "\n",
    "    options_returns.loc[:, \"date\"] = pd.to_datetime(options_returns.loc[:, \"date\"])\n",
    "    options_returns = options_returns.rename(columns={\"return\": \"option_returns\"})\n",
    "    options_returns = options_returns.set_index(\"date\")\n",
    "\n",
    "    temp1 = (\n",
    "        options_returns.groupby(by=[\"secid\"])[\"option_returns\"].rolling(window=21).var()\n",
    "        * 252\n",
    "    )\n",
    "    temp1.name = \"realized_variance\"\n",
    "    temp1 = temp1.reset_index()\n",
    "    db.close()\n",
    "\n",
    "    df_stocks_betas_options = df_stocks_betas_options.merge(\n",
    "        temp1, how=\"left\", on=[\"date\", \"secid\"]\n",
    "    )\n",
    "    df_stocks_betas_options.loc[:, \"vrp\"] = (\n",
    "        df_stocks_betas_options.loc[:, \"iv\"] ** 2\n",
    "    ).rolling(21).sum() / (df_stocks_betas_options.loc[:, \"realized_variance\"]).rolling(\n",
    "        21\n",
    "    ).sum()\n",
    "    df_stocks_betas_options.drop([\"realized_variance\"], inplace=True, axis=1)\n",
    "\n",
    "    df_stocks_betas_options.to_hdf(\n",
    "        hdf_path, key=\"factors\", mode=\"a\", data_columns=True, complib=\"zlib\"\n",
    "    )\n",
    "    return df_stocks_betas_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_betas_options = get_vrp(wrds_username=wrds_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def technicals_rock(df_a):\n",
    "    KAMA = KAMAIndicator(close = df_a[\"prc\"],window = 10,pow1 = 2, pow2 = 30)\n",
    "    PPO = PercentagePriceOscillator(close = df_a[\"prc\"],window_slow = 26, window_fast = 12, window_sign = 9)\n",
    "    ROCI = ROCIndicator(close=df_a[\"prc\"],window=12)\n",
    "    RSI = RSIIndicator(close=df_a[\"prc\"],window = 14)\n",
    "    EMA14 = EMAIndicator(close = df_a[\"prc\"],window = 14)\n",
    "    EMA25 = EMAIndicator(close = df_a[\"prc\"],window = 25)\n",
    "    MACD_ind = MACD(close = df_a[\"prc\"],window_slow = 26,window_fast = 12,window_sign=9)\n",
    "    Aroon = AroonIndicator(close = df_a[\"prc\"],window = 25)\n",
    "    \n",
    "    df_a = df_a.assign(    \n",
    "    KAMA = KAMA.kama(),\n",
    "    PPO = PPO.ppo_signal(),\n",
    "    ROCI = ROCI.roc(),\n",
    "    RSI = RSI.rsi(),\n",
    "    EMA14 = EMA14.ema_indicator(),\n",
    "    EMA25 = EMA25.ema_indicator(),\n",
    "    MACD = MACD_ind.macd_signal(),\n",
    "    AroonInd = Aroon.aroon_indicator()\n",
    "    )\n",
    "    \n",
    "    return df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_betas_options = technicals_rock(df_stocks_betas_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Columns with Too Many Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan(df):\n",
    "    \"\"\"\n",
    "    This function counts None values on each column plots the count as a bar-chart.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input: dataframe\n",
    "        The dataframe with the columns for which the count of None values shall be plotted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the plotted None values as a bar-chart.\n",
    "    \"\"\"\n",
    "    # Count NaN\n",
    "    df_count_nan = df.isna().sum().reset_index(name=\"Count NaN\")\n",
    "    # Rename column\n",
    "    df_count_nan = df_count_nan.rename(columns={\"index\": \"Column\"})\n",
    "    # Plot\n",
    "    df_count_nan.plot.bar(\n",
    "        x=\"Column\", y=\"Count NaN\", rot=45, figsize=(16, 6), title=\"Count NaN per Column\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan(df=df_stocks_betas_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with many nas\n",
    "df_stocks_betas_options.drop(\n",
    "    [\n",
    "        \"glb2_D182\",\n",
    "        \"glb3_D182\",\n",
    "        \"glb2_D273\",\n",
    "        \"glb3_D273\",\n",
    "        \"glb2_D365\",\n",
    "        \"glb3_D365\",\n",
    "        \"mfis182\",\n",
    "        \"mfis273\",\n",
    "        \"mfis365\",\n",
    "    ],\n",
    "    inplace=True,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "Multicollinearity occurs when there are two or more independent variables in a multiple regression model, which have a high correlation among themselves (see also [here](https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/) and [here](https://www.investopedia.com/terms/m/multicollinearity.asp)). This may lead to difficulties, when trying to distinguis between their individual effects on the dependent variable. <br> \n",
    "In statistics, one commone approach to quantify a relationship between two variables is to the [Pearson correlation coefficient](https://www.statology.org/pearson-correlation-coefficient/), which is a measure of the linear association between two variables. It has a value between -1 and 1 where:\n",
    "- -1 indicates a perfectly negative linear correlation between two variables\n",
    "- 0 indicates no linear correlation between two variables\n",
    "- 1 indicates a perfectly positive linear correlation between two variables\n",
    "\n",
    "The further away the Pearson correlation coefficient is away from zero, the stronger the relationship between the two variables.\n",
    "The pairwise Pearson correlation coefficient of the respective variables can be clearly visualized using the correlation matrix as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrmatrix_heatmap(df, columns):\n",
    "    \"\"\"\n",
    "    This function calculates the pairwise Pearson correlation coefficient of the respective variables\n",
    "    and plots them in a heatmap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: dataframe\n",
    "        The dataframe storing the information about the stocks.\n",
    "        \n",
    "    columns: list\n",
    "        The variables for which the Pearson correlation coefficient shall be calculated.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns the heatmap of the pairwise Pearson correlation coefficient. \n",
    "    \"\"\"\n",
    "    \n",
    "    X = df[columns]\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corrMatrix = X.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(14, 11))\n",
    "\n",
    "    sns.heatmap(corrMatrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns to calculate pearson correlation coefficient for\n",
    "idx = list(df_stocks_betas_options.columns).index(\"excess_return\")\n",
    "columns = list(df_stocks_betas_options.columns)[idx+1:]\n",
    "\n",
    "# execute function\n",
    "corrmatrix_heatmap(df=df_stocks_betas_options, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we use the [variance inflation factor (VIF)](https://www.statsmodels.org/devel/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html) to additionally measure the amount of collinearity. One recommendation is that if VIF is greater than 5, then the explanatory variable given by exog_idx is highly collinear with the other explanatory variables, and the parameter estimates will have large standard errors because of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VIF_Features(df):\n",
    "    \"\"\"\n",
    "    This function calculates the variance inflation factor (VIF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: dataframe\n",
    "        The dataframe storing the information about the stocks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    This functions returns variance inflation factors.\n",
    "    \"\"\"\n",
    "\n",
    "    X_ = df.copy()\n",
    "    X_ = X_[X_.columns[10:]]\n",
    "    X_ = X_.dropna()\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_.columns\n",
    "    vif_data[\"VIF\"] = [\n",
    "        variance_inflation_factor(X_.values, i) for i in range(len(X_.columns))\n",
    "    ]\n",
    "\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_stocks_betas_options.drop(\n",
    "    [\"glb2_D30\", \"glb2_D91\", \"glb3_D91\", \"mfis30\", \"EMA25\", \"EMA14\", \"KAMA\"],\n",
    "    inplace=True,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = VIF_Features(df_stocks_betas_options)\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_returns(df, kind):\n",
    "    if kind == \"Alpha\":\n",
    "        benchmark_returns = df.groupby([\"date\"])[[\"weightage_pct\", \"ret\"]].apply(\n",
    "            lambda x: np.nansum(x[\"ret\"] * x[\"weightage_pct\"])\n",
    "        )\n",
    "        benchmark_returns.name = \"ret_benchmark\"\n",
    "        df = df.merge(benchmark_returns, on=[\"date\"])\n",
    "        df[\"ret_bm_fw\"] = df.groupby([\"permno\"])[\"ret_benchmark\"].shift(-1)\n",
    "        df.loc[:, \"fret1d\"] = df.groupby([\"permno\"])[\"ret\"].shift(-1)\n",
    "        df[\"fret1d\"] = df[\"fret1d\"] - df[\"ret_bm_fw\"]\n",
    "        df.drop(columns={\"ret_bm_fw\", \"ret_benchmark\"}, inplace=True)\n",
    "    if kind == \"Regular\":\n",
    "        df.loc[:, \"fret1d\"] = df.groupby([\"permno\"])[\"ret\"].shift(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_betas_options = alpha_returns(df_stocks_betas_options, kind=\"Regular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_sorted(df, date_of_split, dates):\n",
    "\n",
    "    date_of_split = pd.to_datetime(date_of_split)\n",
    "    X_train = df[\n",
    "        (df[\"date\"] <= date_of_split) & (df[\"date\"] > pd.datetime(1999, 12, 31))\n",
    "    ]\n",
    "    X_test = df[(df[\"date\"] > date_of_split) & (df[\"date\"] < pd.datetime(2020, 1, 1))]\n",
    "\n",
    "    X_train = X_train.sort_values(by=[\"date\", \"permno\"])\n",
    "    X_test = X_test.sort_values(by=[\"date\", \"permno\"])\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df, columns):\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    df_scaled = df_scaled.fillna(method=\"ffill\", limit=30, inplace=False)\n",
    "    df_scaled = df_scaled.dropna()\n",
    "\n",
    "    for column in columns:\n",
    "        df_scaled[column] = (df_scaled[column] - df_scaled[column].min())  / (df_scaled[column].max() - df_scaled[column].min())\n",
    "        df_scaled[column], fitted_lambda = stats.boxcox(df_scaled[column] + 0.0001) \n",
    "        df_scaled.loc[:,column] = winsorize(df_scaled.loc[:,column], limits=0.01).data\n",
    "        df_scaled[column] -= df_scaled[column].mean()\n",
    "        \n",
    "    return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_betas_options.loc[:, \"date\"] = pd.to_datetime(\n",
    "    df_stocks_betas_options.loc[:, \"date\"]\n",
    ")\n",
    "\n",
    "train, test = train_test_split_sorted(\n",
    "    df=df_stocks_betas_options,\n",
    "    date_of_split=pd.datetime(2012, 12, 31),\n",
    "    dates=df_stocks_betas_options.date,\n",
    ")\n",
    "\n",
    "columns2norm = train.columns[10:-1]\n",
    "\n",
    "# Only the train gets processed, the rest will be processed on a rolling window in the testing phase\n",
    "train = process(train, columns2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"skew1\",\n",
    "    \"skew2\",\n",
    "    \"iv\",\n",
    "    \"RSI\",\n",
    "    \"mktrf_beta_eom\",\n",
    "    \"smb_beta_eom\",\n",
    "    \"mom_beta_eom\",\n",
    "    \"hml_beta_eom\",\n",
    "    \"yield_dif_10y_1y_beta_eom\",\n",
    "] \n",
    "fig, axs = plt.subplots(3, 3)\n",
    "fig.set_size_inches(18.5, 12.5, forward=True)\n",
    "fig.suptitle(\"Distribution of Variables\", fontsize=18)\n",
    "\n",
    "for i, el in enumerate(cols):\n",
    "    a = train.hist(el, ax=axs.flatten()[i], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_forest(data, predictors, target, window, params):\n",
    "    # First we form the window\n",
    "    start_window = data.date.min()\n",
    "    end_window = data.date.min() + BDay(window)\n",
    "    pred_date = end_window + BDay(1)\n",
    "\n",
    "    # Auxiliary placeholders and reg. constant\n",
    "    data = data.sort_values(by=[\"date\", \"permno\"])\n",
    "    predictions_df = pd.DataFrame(\n",
    "        index=[pred_date], columns=np.sort(data.permno.unique())\n",
    "    )\n",
    "    scores = []\n",
    "    missing_dates = []\n",
    "\n",
    "    # The train / predict loop\n",
    "    while pred_date <= data.date.max():\n",
    "        # The loop will go through business days, not necessarily trading days (hence the try clause).\n",
    "        try:\n",
    "            # Picking the window of data to train and instantiating the predictor model\n",
    "            mask = (data[\"date\"] >= start_window) & (data[\"date\"] <= end_window)\n",
    "            train_window = data[mask].set_index(\"permno\")\n",
    "            linreg = RandomForestRegressor(**params)\n",
    "            # Fitting the model on the training window\n",
    "            fit_ = linreg.fit(train_window[list(predictors)], train_window[target])\n",
    "            # Predicting for each permno on the next day after fit\n",
    "            predi_df = (\n",
    "                data[data[\"date\"] == pred_date]\n",
    "                .loc[:, list(predictors) + [\"permno\"]]\n",
    "                .set_index(\"permno\")\n",
    "            )\n",
    "            predi = fit_.predict(predi_df)\n",
    "            # Storing in a useful format for backtest\n",
    "            one_predi = (\n",
    "                pd.DataFrame(data=predi, index=predi_df.index, columns=[pred_date])\n",
    "            ).T\n",
    "            predictions_df = pd.concat(\n",
    "                [predictions_df, one_predi], axis=0, ignore_index=False\n",
    "            )\n",
    "            # R^2 to help visualize training performance\n",
    "            score_r2 = linreg.score(\n",
    "                predi_df[list(predictors)], data[data[\"date\"] == pred_date][target]\n",
    "            )\n",
    "            scores.append(score_r2)\n",
    "        except:\n",
    "            missing_dates.append(pred_date)\n",
    "        # Update window position\n",
    "        start_window = start_window + BDay(1)\n",
    "        end_window = start_window + BDay(window)\n",
    "        pred_date = end_window + BDay(1)\n",
    "    # Real returns we will need\n",
    "    real_returns = data.pivot(index=\"date\", columns=\"permno\", values=\"fret1d\")\n",
    "\n",
    "    return real_returns, predictions_df[1:], missing_dates, scores  # , r2_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": [10, 20, 30],\n",
    "    \"max_features\": [\"sqrt\", \"auto\", \"log2\"],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4],\n",
    "    \"min_samples_split\": [2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "train.insert(10, \"constant\", 1.0)\n",
    "regressors = train.columns[10:-1]\n",
    "target = [\"fret1d\"]\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "\n",
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    forest,\n",
    "    param_distributions=params,\n",
    "    n_iter=param_comb,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=4,\n",
    "    cv=KFold(5),\n",
    "    verbose=3,\n",
    "    random_state=1001,\n",
    ")\n",
    "random_search.fit(train[regressors], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_forest = {\n",
    "    \"n_estimators\": 30,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 3,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"random_state\": 1001,\n",
    "}\n",
    "\n",
    "regressors = train.columns[10:-1]\n",
    "target = [\"fret1d\"]\n",
    "\n",
    "(\n",
    "    real_returns_forest,\n",
    "    expected_returns_forest,\n",
    "    missing_dates_forest,\n",
    "    scores_forest,\n",
    ") = rolling_forest(\n",
    "    data=train, predictors=regressors, target=target, window=126, params=params_forest\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tune the alpha on the whole train set and then again apply a rolling approach to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(data, predictors, target, alpha):\n",
    "    lasso = Lasso(alpha=alpha,normalize=True, max_iter=1e5).fit(data[predictors],data[target])\n",
    "    predi = lasso.predict(data[predictors]) \n",
    "    score=lasso.score(data[predictors],data[target])\n",
    "    n = np.sum(lasso.coef_!=0)\n",
    "    mse=metrics.mean_squared_error(data[target].to_numpy(), predi)\n",
    "    \n",
    "    return mse, score,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lasso = [1e-16, 1e-8, 1e-4, 1e-2, 1, 10]\n",
    "res = pd.DataFrame(index=alpha_lasso, columns=[\"MSE\"])\n",
    "for i in range(len(alpha_lasso)):\n",
    "    (\n",
    "        res.loc[alpha_lasso[i], \"MSE\"],\n",
    "        res.loc[alpha_lasso[i], \"Score\"],\n",
    "        res.loc[alpha_lasso[i], \"No.\"],\n",
    "    ) = lasso_regression(train, regressors, target, alpha_lasso[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_lasso(data, predictors, target, window, alpha):\n",
    "\n",
    "    # First we form the window\n",
    "    start_window = data.date.min()\n",
    "    end_window = data.date.min() + BDay(window)\n",
    "    pred_date = end_window + BDay(1)\n",
    "\n",
    "    # Auxiliary placeholders and reg. constant\n",
    "    data = data.sort_values(by=[\"date\", \"permno\"])\n",
    "    predictions_df = pd.DataFrame(\n",
    "        index=[pred_date], columns=np.sort(data.permno.unique())\n",
    "    )\n",
    "    scores = []\n",
    "    missing_dates = []\n",
    "\n",
    "    # The train / predict loop\n",
    "    while pred_date <= data.date.max():\n",
    "        # The loop will go through business days, not necessarily trading days (hence the try clause).\n",
    "        try:\n",
    "            # Picking the window of data to train and instantiating the predictor model\n",
    "            mask = (data[\"date\"] >= start_window) & (data[\"date\"] <= end_window)\n",
    "            train_window = data[mask].set_index(\"permno\")\n",
    "            linreg = Lasso(alpha=alpha, normalize=True, max_iter=1e5)\n",
    "            # Fitting the model on the training window\n",
    "            fit_ = linreg.fit(train_window[list(predictors)], train_window[target])\n",
    "            # Predicting for each permno on the next day after fit\n",
    "            predi_df = (\n",
    "                data[data[\"date\"] == pred_date]\n",
    "                .loc[:, list(predictors) + [\"permno\"]]\n",
    "                .set_index(\"permno\")\n",
    "            )\n",
    "            predi = fit_.predict(predi_df)\n",
    "            # Storing in a useful format for backtest\n",
    "            one_predi = (\n",
    "                pd.DataFrame(data=predi, index=predi_df.index, columns=[pred_date])\n",
    "            ).T\n",
    "            predictions_df = pd.concat(\n",
    "                [predictions_df, one_predi], axis=0, ignore_index=False\n",
    "            )\n",
    "            # R^2 to help visualize training performance\n",
    "            score_r2 = linreg.score(\n",
    "                predi_df[list(predictors)], data[data[\"date\"] == pred_date][target]\n",
    "            )\n",
    "            scores.append(score_r2)\n",
    "        except:\n",
    "            missing_dates.append(pred_date)\n",
    "        # Update window position\n",
    "        start_window = start_window + BDay(1)\n",
    "        end_window = start_window + BDay(window)\n",
    "        pred_date = end_window + BDay(1)\n",
    "    # Real returns we will need\n",
    "    real_returns = data.pivot(index=\"date\", columns=\"permno\", values=\"fret1d\")\n",
    "\n",
    "    return real_returns, predictions_df[1:], missing_dates, scores  # , r2_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    real_returns_lasso,\n",
    "    expected_returns_lasso,\n",
    "    missing_dates_lasso,\n",
    "    scores_lasso,\n",
    ") = rolling_lasso(\n",
    "    data=train, predictors=regressors, target=target, window=126, alpha=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_returns_forest.to_hdf(\n",
    "    hdf_path, key=\"Predictions\", mode=\"a\", data_columns=True, complib=\"zlib\"\n",
    ")\n",
    "real_returns_forest.to_hdf(\n",
    "    hdf_path, key=\"Real\", mode=\"a\", data_columns=True, complib=\"zlib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_returns = pd.read_hdf(hdf_path, key=\"Real\")\n",
    "expected_returns = pd.read_hdf(hdf_path, key=\"Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_returns(real, expected, days_to_test,years_lookback):\n",
    "    real = real.loc[np.logical_and(real.index >= days_to_test - pd.Timedelta(days=252*years_lookback + 1), real.index < days_to_test), :]\n",
    "    expected = expected.loc[np.logical_and(expected.index >= days_to_test - pd.Timedelta(days=252*years_lookback + 1), expected.index < days_to_test), :]   \n",
    "    real = real.fillna(0.0)\n",
    "    expected = expected.fillna(0.0)\n",
    "    \n",
    "    return real, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real,ret = prepare_returns(real_returns,expected_returns,pd.datetime(2012, 12, 28),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MVP(wvec, *args):\n",
    "     cov = args[0]\n",
    "     var = wvec@cov@wvec\n",
    "     return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_drawdown_rolling(df, ret, d, lookback):\n",
    "    d = pd.Timestamp(d)\n",
    "    start = d - BDay(lookback)\n",
    "\n",
    "    means = ret.loc[start : (d - BDay(1)), :].values\n",
    "    means_df = ret.loc[start : (d - BDay(1)), :]\n",
    "\n",
    "    weights_bench = df[(df[\"date\"] >= start) & (df[\"date\"] < d)]\n",
    "    weights_bench = weights_bench[[\"date\", \"permno\", \"weightage_pct\"]]\n",
    "    weights_bench = weights_bench.pivot(\n",
    "        index=\"date\", columns=\"permno\", values=\"weightage_pct\"\n",
    "    )\n",
    "    weights_bench = weights_bench.loc[weights_bench.index.isin(means_df.index)]\n",
    "    monthly_benchmark_returns = (means * weights_bench).sum(axis=1)\n",
    "    monthly_benchmark_returns = pd.DataFrame(\n",
    "        monthly_benchmark_returns, columns=[\"Benchmark_Return\"]\n",
    "    )\n",
    "\n",
    "    # cumulative return\n",
    "    monthly_benchmark_returns[\"Cum_ret\"] = (\n",
    "        1 + monthly_benchmark_returns[\"Benchmark_Return\"]\n",
    "    ).cumprod()\n",
    "\n",
    "    # cumulative peaks\n",
    "    monthly_benchmark_returns[\"Peaks\"] = monthly_benchmark_returns[\"Cum_ret\"].cummax()\n",
    "    \n",
    "    # drawdown from trailing peak\n",
    "    monthly_benchmark_returns[\"Drawdown\"] = (\n",
    "        monthly_benchmark_returns[\"Cum_ret\"] - monthly_benchmark_returns[\"Peaks\"]\n",
    "    ) / monthly_benchmark_returns[\"Peaks\"]\n",
    "\n",
    "    return monthly_benchmark_returns, monthly_benchmark_returns[\"Drawdown\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_inputs_for_optimisation(df, ret, expret, d, limit, lookback):\n",
    "\n",
    "    cov = ret.cov().values\n",
    "    mu = expret.iloc[-1].values\n",
    "    mu = mu - np.mean(mu)\n",
    "    mu = winsorize(mu, limits=0.1).data\n",
    "\n",
    "    if np.sum(mu > 0) == 0:\n",
    "        mu -= mu.mean()\n",
    "\n",
    "    # Bounds on Weights Based on Benchmark on day d\n",
    "    bounds = [\n",
    "        [] for _ in range(len(ret.columns))\n",
    "    ]  # Empty placeholder for the 50 lower and upper bounds per day\n",
    "    for en, i in enumerate(ret.columns, start=0):\n",
    "        try:\n",
    "            lower_bound = 0.9 * float(\n",
    "                df.loc[df[\"date\"] == d][df[\"permno\"] == i].weightage_pct.values\n",
    "            )\n",
    "            higher_bound = 1.1 * float(\n",
    "                df.loc[df[\"date\"] == d][df[\"permno\"] == i].weightage_pct.values\n",
    "            )\n",
    "            bounds[en].extend((lower_bound, higher_bound))\n",
    "        except:\n",
    "            lower_bound = 0.0\n",
    "            higher_bound = 0.0\n",
    "            bounds[en].extend((lower_bound, higher_bound))\n",
    "\n",
    "    # Constraints on Factor Exposure\n",
    "    cons = []\n",
    "    fact = pd.DataFrame(columns=list(regressors) + [\"permno\"])\n",
    "    fact.permno = ret.columns.tolist()\n",
    "    fact.set_index(\"permno\", inplace=True)\n",
    "    fact = fact.fillna(0.0)\n",
    "\n",
    "    # Exposure of strategy\n",
    "    for stock in fact.index:\n",
    "        for fct in fact.columns:\n",
    "            flt = np.logical_and(df[\"date\"] == d, df[\"permno\"] == stock)\n",
    "            if flt.any():\n",
    "                fact.loc[stock, fct] = df.loc[flt, fct].values[0]\n",
    "\n",
    "    B = fact.T[1:]\n",
    "\n",
    "    # Benchmark Weights\n",
    "    weights_Benchmark = df.loc[df[\"date\"] == d][[\"permno\", \"weightage_pct\"]]\n",
    "    weights_Benchmark = weights_Benchmark.sort_values(\"permno\")\n",
    "    weights_Benchmark = weights_Benchmark.set_index([\"permno\"]).squeeze()\n",
    "    for i in df.permno.unique():\n",
    "        if i not in weights_Benchmark.index:\n",
    "            weights_Benchmark[i] = 0.0\n",
    "    weights_Benchmark = weights_Benchmark.sort_index()\n",
    "\n",
    "    cons.append(\n",
    "        {\"type\": \"ineq\", \"fun\": lambda wvec: B @ weights_Benchmark - B @ wvec + limit}\n",
    "    )\n",
    "    cons.append(\n",
    "        {\"type\": \"ineq\", \"fun\": lambda wvec: B @ wvec - B @ weights_Benchmark + limit}\n",
    "    )\n",
    "\n",
    "    # Constraints on Weight Sum (long portfolio)\n",
    "    cons.append({\"type\": \"eq\", \"fun\": lambda wvec: wvec.sum() - 1})\n",
    "\n",
    "    # Constraints on Drawdown\n",
    "    if (pd.Timestamp(d) - BDay(lookback)) in ret.index:\n",
    "        _, benchmark_dd = benchmark_drawdown_rolling(df, ret, d, 21)\n",
    "        cons.append(\n",
    "            {\"type\": \"ineq\", \"fun\": lambda wvec: mu.T @ wvec - (benchmark_dd - 0.01)}\n",
    "        )\n",
    "\n",
    "    return cov, mu, cons, bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest on In-Sample Period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtall = pd.DataFrame(index=ret.index, data=ret.index, columns=[\"dates\"])  #\n",
    "dt_m = dtall.groupby(\n",
    "    pd.Grouper(freq=\"B\")\n",
    ").last()  # We can rebalane daily for final solution should be better results\n",
    "dates_rebal = dt_m.index.values  # But less realistic (daily trading)\n",
    "\n",
    "# We need to also exclude from rebalance dates the business days that were actually not trading days\n",
    "dates_rebal = [x for x in dates_rebal if x not in missing_dates_forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ret.columns\n",
    "ptf = {}\n",
    "\n",
    "# Initiate a dictionary of placeholders for the 3 strategies\n",
    "ptf[\"msr_w_cons\"] = pd.DataFrame(data=0.0, index=dates_rebal, columns=names)\n",
    "ptf[\"minvar_w_cons\"] = pd.DataFrame(data=0.0, index=dates_rebal, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optimization routine for the minimization problem\n",
    "st_point_mvp = None\n",
    "\n",
    "for d in tqdm(dates_rebal):\n",
    "    print(d)\n",
    "    start_time = time.time()\n",
    "\n",
    "    ret, expret = prepare_returns(real_returns, expected_returns, d, 3)\n",
    "\n",
    "    # we prepare inputs:\n",
    "    cov, mu, cons, bounds = configure_inputs_for_optimisation(\n",
    "        train, ret, expret, d, 0.05, 21\n",
    "    )\n",
    "\n",
    "    # define starting point\n",
    "    if st_point_msr is None:\n",
    "        st_point_msr = np.ones(ret.columns.size) / ret.columns.size\n",
    "    if st_point_mvp is None:\n",
    "        st_point_mvp = np.ones(ret.columns.size) / ret.columns.size\n",
    "\n",
    "    res = minimize(\n",
    "        MVP,\n",
    "        st_point_mvp,\n",
    "        args=(cov, mu),\n",
    "        constraints=cons,\n",
    "        bounds=bounds,\n",
    "        method=\"SLSQP\",\n",
    "        options={\"ftol\": 1e-16, \"disp\": False},\n",
    "    )\n",
    "    ptf[\"minvar_w_cons\"].loc[d] = res.x\n",
    "    # Set next starting point\n",
    "    st_point_mvp = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clc_str_return(w, ret, str_name):\n",
    "    w_i = np.zeros(len(ret.columns.tolist()))\n",
    "    ret_str = pd.DataFrame(index = ret.index, columns=[str_name])\n",
    "    for i in ret.index:\n",
    "        ret_str.loc[i, str_name] = w_i@ret.loc[i,:]\n",
    "        if i in w.index:\n",
    "            w_i = w.loc[i,:]\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_str = clc_str_return(ptf[\"msr_w_cons\"], ret, \"msr_w_cons\")\n",
    "ret_str = ret_str.join(clc_str_return(ptf[\"minvar_w_cons\"], ret, \"minvar_w_cons\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest on Out-of-Sample Period \n",
    "\n",
    "The function is only different so that it also normalizes the rolling window we are fitting/testing on. This is important so we avoid lookahead bias in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_forest_test(data, predictors, target, window, params):\n",
    "    # First we form the window\n",
    "    start_window = data.date.min()\n",
    "    end_window = data.date.min() + BDay(window)\n",
    "    pred_date = end_window + BDay(1)\n",
    "\n",
    "    # Auxiliary placeholders and reg. constant\n",
    "    data = data.sort_values(by=[\"date\", \"permno\"])\n",
    "    predictions_df = pd.DataFrame(\n",
    "        index=[pred_date], columns=np.sort(data.permno.unique())\n",
    "    )\n",
    "    scores = []\n",
    "    missing_dates = []\n",
    "\n",
    "    # The train / predict loop\n",
    "    while pred_date <= data.date.max():\n",
    "        # The loop will go through business days, not necessarily trading days (hence the try clause).\n",
    "        try:\n",
    "            # Picking the window of data to train and instantiating the predictor model\n",
    "            mask = (data[\"date\"] >= start_window) & (data[\"date\"] <= end_window)\n",
    "            train_window = data[mask].set_index(\"permno\")\n",
    "            train_window = process(train_window, list(predictors)[1:])\n",
    "            linreg = RandomForestRegressor(**params)\n",
    "            # Fitting the model on the training window\n",
    "            fit_ = linreg.fit(train_window[list(predictors)], train_window[target])\n",
    "            # Predicting for each permno on the next day after fit\n",
    "            predi_df = (\n",
    "                data[data[\"date\"] == pred_date]\n",
    "                .loc[:, list(predictors) + [\"permno\"]]\n",
    "                .set_index(\"permno\")\n",
    "            )\n",
    "            predi = fit_.predict(predi_df)\n",
    "            # Storing in a useful format for backtest\n",
    "            one_predi = (\n",
    "                pd.DataFrame(data=predi, index=predi_df.index, columns=[pred_date])\n",
    "            ).T\n",
    "            predictions_df = pd.concat(\n",
    "                [predictions_df, one_predi], axis=0, ignore_index=False\n",
    "            )\n",
    "            # R^2 to help visualize training performance\n",
    "            score_r2 = linreg.score(\n",
    "                predi_df[list(predictors)], data[data[\"date\"] == pred_date][target]\n",
    "            )\n",
    "            scores.append(score_r2)\n",
    "        except:\n",
    "            missing_dates.append(pred_date)\n",
    "        # Update window position\n",
    "        start_window = start_window + BDay(1)\n",
    "        end_window = start_window + BDay(window)\n",
    "        pred_date = end_window + BDay(1)\n",
    "    # Real returns we will need\n",
    "    real_returns = data.pivot(index=\"date\", columns=\"permno\", values=\"fret1d\")\n",
    "\n",
    "    return real_returns, predictions_df[1:], missing_dates, scores  # , r2_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_forest = {\n",
    "    \"n_estimators\": 30,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 3,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"random_state\": 1001,\n",
    "}\n",
    "\n",
    "regressors = test.columns[10:-1]\n",
    "target = [\"fret1d\"]\n",
    "\n",
    "(\n",
    "    real_returns_forest_test,\n",
    "    expected_returns_forest_test,\n",
    "    missing_dates_forest_test,\n",
    "    scores_forest_test,\n",
    ") = rolling_forest_test(\n",
    "    data=test, predictors=regressors, target=target, window=21, params=params_forest\n",
    ")\n",
    "real_returns_forest_test = real_returns_forest_test[\n",
    "    real_returns_forest_test.index >= expected_returns_forest_test.index.min()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, expret = prepare_returns(real_returns_forest_test,expected_returns_forest_test,pd.datetime(2019, 12, 31),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtall = pd.DataFrame(index=ret.index, data=ret.index, columns=[\"dates\"])  #\n",
    "dt_m = dtall.groupby(\n",
    "    pd.Grouper(freq=\"B\")\n",
    ").last()  # We can rebalane daily for final solution should be better results\n",
    "dates_rebal = dt_m.index.values  # But less realistic (daily trading)\n",
    "\n",
    "# We need to also exclude from rebalance dates the business days that were actually not trading days\n",
    "# #missing_dates = [i.to_datetime64() for i in missing_dates] # Uncomment if needed a convert!\n",
    "dates_rebal = [x for x in dates_rebal if x not in missing_dates_forest_test]\n",
    "\n",
    "# dates_ = [i.date() for i in dates_rebal]\n",
    "real_returns_forest_test = real_returns_forest_test.loc[\n",
    "    ~real_returns_forest_test.index.isin(missing_dates_forest_test)\n",
    "]\n",
    "expected_returns_forest_test = expected_returns_forest_test.loc[\n",
    "    ~expected_returns_forest_test.index.isin(missing_dates_forest_test)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ret.columns\n",
    "ptf = {}\n",
    "\n",
    "# Initiate a dictionary of placeholders for the 3 strategies\n",
    "ptf[\"msr_w_cons\"] = pd.DataFrame(data=0.0, index=dates_rebal, columns=names)\n",
    "ptf[\"minvar_w_cons\"] = pd.DataFrame(data=0.0, index=dates_rebal, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnch_ret = (\n",
    "    df_betas.groupby([\"date\"]).apply(weighted_avg, \"ret\", \"weightage_pct\") + 1\n",
    ").cumprod()\n",
    "\n",
    "bnch_ret = bnch_ret.loc[\n",
    "    (bnch_ret.index >= ret_str.index.min()) & (bnch_ret.index <= ret_str.index.max())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optimization rutine for the minimization problem\n",
    "st_point_mvp = None\n",
    "\n",
    "for d in tqdm(dates_rebal):\n",
    "    ret, expret = prepare_returns(\n",
    "        real_returns_forest_test, expected_returns_forest_test, d, 3\n",
    "    )\n",
    "\n",
    "    # we prepare inputs:\n",
    "    cov, mu, cons, bounds = configure_inputs_for_optimisation(\n",
    "        test, ret, expret, d, 0.05, 21\n",
    "    )\n",
    "\n",
    "    # define starting point\n",
    "    if st_point_msr is None:\n",
    "        st_point_msr = np.ones(ret.columns.size) / ret.columns.size\n",
    "    if st_point_mvp is None:\n",
    "        st_point_mvp = np.ones(ret.columns.size) / ret.columns.size\n",
    "\n",
    "    res = minimize(\n",
    "        MVP,\n",
    "        st_point_mvp,\n",
    "        args=(cov, mu),\n",
    "        constraints=cons,\n",
    "        bounds=bounds,\n",
    "        method=\"SLSQP\",\n",
    "        options={\"ftol\": 1e-16, \"disp\": False},\n",
    "    )\n",
    "\n",
    "    ptf[\"minvar_w_cons\"].loc[d] = res.x\n",
    "\n",
    "    # Set next starting point\n",
    "    st_point_mvp = res.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, expret = prepare_returns(\n",
    "    real_returns_forest_test, expected_returns_forest_test, pd.datetime(2019, 12, 31), 9\n",
    ")\n",
    "ret_str = clc_str_return(ptf[\"msr_w_cons\"], ret, \"msr_w_cons\")\n",
    "ret_str = ret_str.join(clc_str_return(ptf[\"minvar_w_cons\"], ret, \"minvar_w_cons\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs.reports.full(ret_str[\"minvar_w_cons\"], bnch_ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
